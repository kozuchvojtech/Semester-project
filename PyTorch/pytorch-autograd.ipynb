{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "tensor1 = torch.Tensor([[1,2,3],[4,5,6]])\n",
    "tensor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 7.,  8.,  9.],\n",
       "        [10., 11., 12.]])"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "tensor2 = torch.Tensor([[7,8,9],[10,11,12]])\n",
    "tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# true -> PyTorch tracks computations for a tensor in the forward phase and will calculate gradients for this tensor in the backward phase\n",
    "# we need to enabled it so the history is tracked for the tensor and gradients are calculated with respect to the tensor\n",
    "tensor1.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "tensor1.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "tensor1.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# no gradients available yet -> though it's part of a computation graph, no forward or backward passes been made yet\n",
    "print(tensor1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# tensors and functions in a computation graph (tensors = nodes; functions = edges)\n",
    "print(tensor1.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "output_tensor = tensor1 * tensor2\n",
    "output_tensor.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<MulBackward0 object at 0x0000022DEE05AD90>\n"
     ]
    }
   ],
   "source": [
    "# grad fn is \"multiplication\" backward (because tensor1 * tensor2)\n",
    "print(output_tensor.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<MeanBackward0 object at 0x0000022DEE53AA00>\n"
     ]
    }
   ],
   "source": [
    "# though two operations were used, tensor will have the grad fn set to the last one -> mean\n",
    "output_tensor = (tensor1 * tensor2).mean()\n",
    "print(output_tensor.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward computation for the computation graph\n",
    "output_tensor.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1.1667, 1.3333, 1.5000],\n        [1.6667, 1.8333, 2.0000]])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), torch.Size([2, 3]))"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# gradient of tensor1 -> partial derivatives for parameters in tensor1 calculated with reference to the output tensor\n",
    "print(tensor1.grad)\n",
    "# shape of the gradient will match the shape of the tensor\n",
    "tensor1.grad.shape, tensor1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# though the tensor2 was involved in the calc. as well, because of requires_grad set to false -> no gradient\n",
    "print(tensor2.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new_tensor =  tensor([[ 3.,  6.,  9.],\n        [12., 15., 18.]])\nrequires_grad for tensor =  True\nrequires_grad for tensor =  False\nrequires_grad for tensor =  False\n"
     ]
    }
   ],
   "source": [
    "# torch.no_grad() to stop autograd from tracking the history on those tensors -> every computation within the with block with no history\n",
    "with torch.no_grad():\n",
    "    new_tensor = tensor1 * 3\n",
    "    print('new_tensor = ', new_tensor)\n",
    "    print('requires_grad for tensor = ', tensor1.requires_grad)\n",
    "    print('requires_grad for tensor = ', tensor2.requires_grad)\n",
    "    print('requires_grad for tensor = ', new_tensor.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(t):\n",
    "    return t * 2\n",
    "\n",
    "# decorater @torch.no_grad() -> gradients won't be generated; no tracking history, even though the tensors require it\n",
    "@torch.no_grad()\n",
    "def calculate_with_no_grad(t):\n",
    "    return t * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 2.,  4.,  6.],\n",
       "        [ 8., 10., 12.]], grad_fn=<MulBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "result_tensor = calculate(tensor1)\n",
    "result_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 2.,  4.,  6.],\n",
       "        [ 8., 10., 12.]])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "result_tensor_no_grad = calculate_with_no_grad(tensor1)\n",
    "result_tensor_no_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "result_tensor_no_grad.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new_tensor_no_grad =  tensor([[ 3.,  6.,  9.],\n        [12., 15., 18.]])\nnew_tensor_grad =  tensor([[ 3.,  6.,  9.],\n        [12., 15., 18.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    new_tensor_no_grad = tensor1 * 3\n",
    "    print('new_tensor_no_grad = ', new_tensor_no_grad)\n",
    "\n",
    "    with torch.enable_grad():\n",
    "        new_tensor_grad = tensor1 * 3\n",
    "        print('new_tensor_grad = ', new_tensor_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "# the option to specify the requires_grad on tensor declaration\n",
    "tensor_one = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
    "tensor_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[5., 6.],\n",
       "        [7., 8.]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "tensor_two = torch.tensor([[5.0, 6.0], [7.0, 8.0]])\n",
    "tensor_two.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(9., grad_fn=<MeanBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "final_tensor = (tensor_one + tensor_two).mean()\n",
    "final_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\nNone\nNone\n"
     ]
    }
   ],
   "source": [
    "print(final_tensor.requires_grad)\n",
    "print(tensor_one.grad)\n",
    "print(tensor_two.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\ntensor([[0.2500, 0.2500],\n        [0.2500, 0.2500]])\ntensor([[0.2500, 0.2500],\n        [0.2500, 0.2500]])\n"
     ]
    }
   ],
   "source": [
    "final_tensor.backward()\n",
    "\n",
    "print(final_tensor.requires_grad)\n",
    "print(tensor_one.grad)\n",
    "print(tensor_two.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "# detached tensor always requires_grad set to false -> isn't part of the computation graph\n",
    "detached_tensor = tensor_one.detach()\n",
    "detached_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_tensor = (tensor_one + detached_tensor).mean()\n",
    "mean_tensor.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.7500, 0.7500],\n        [0.7500, 0.7500]])\nNone\n"
     ]
    }
   ],
   "source": [
    "print(tensor_one.grad)\n",
    "print(detached_tensor.grad)"
   ]
  }
 ]
}